{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "import os\n",
    "#%pip install scipy\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "N_TARGETS = 30\n",
    "N_Q_TARGETS = 21\n",
    "N_A_TARGETS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "#data import \n",
    "train_df = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test_df = pd.read_csv('data/test.csv').fillna(' ')\n",
    "#display (train_df)\n",
    "print (os.listdir('data'))\n",
    "#dataset = Dataset.from_pandas(train_df)\n",
    "#data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
    "#dataset = load_dataset(\"data\", data_files=data_files)\n",
    "#print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_collator = DefaultDataCollator()\\ntokenizer = XLNetTokenizer.from_pretrained(model_name)\\nconfig = XLNetConfig.from_pretrained(\\n    \"roberta-base\",\\n    num_labels=1,\\n    id2label={ 0: \"ðŸ‘Ž\", 1: \"ðŸ‘\"},\\n)\\nmodel = XLNetLMHeadModel.from_pretrained(\\n    \"roberta-base\",\\n    config=config,\\n)'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, DefaultDataCollator, XLNetConfig, XLNetLMHeadModel, XLNetModel\n",
    "model_name = 'xlnet-base-cased' # 'xlnet-large-cased', 'tiny-xlnet-base-cased', 'jkgrad/xlnet-base-squadv2'\n",
    "\n",
    "\"\"\"data_collator = DefaultDataCollator()\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "config = XLNetConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=1,\n",
    "    id2label={ 0: \"ðŸ‘Ž\", 1: \"ðŸ‘\"},\n",
    ")\n",
    "model = XLNetLMHeadModel.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")\"\"\"\n",
    "#model = XLNetForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset5(Dataset):\n",
    "\n",
    "    def __init__(self, x_features, question_ids, answer_ids, seg_question_ids, \n",
    "                    seg_answer_ids, idxs, targets=None):\n",
    "        self.question_ids = question_ids[idxs].astype(np.int64) #np.long\n",
    "        self.answer_ids = answer_ids[idxs].astype(np.int64)\n",
    "        self.seg_question_ids = seg_question_ids[idxs].astype(np.int64)\n",
    "        self.seg_answer_ids = seg_answer_ids[idxs].astype(np.int64)\n",
    "        self.x_features = x_features[idxs].astype(np.float32)\n",
    "        if targets is not None: self.targets = targets[idxs].astype(np.float32)\n",
    "        else: self.targets = np.zeros((self.x_features.shape[0], N_TARGETS), dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_ids = self.question_ids[idx]\n",
    "        a_ids = self.answer_ids[idx]\n",
    "        seg_q_ids = self.seg_question_ids[idx]\n",
    "        seg_a_ids = self.seg_answer_ids[idx]\n",
    "        x_feats = self.x_features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return (x_feats, q_ids, a_ids, seg_q_ids, seg_a_ids), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_features)\n",
    "#features\n",
    "def get_categorical_features(train, test, feature):\n",
    "    unique_vals = list(set(train[feature].unique().tolist() \n",
    "                            + test[feature].unique().tolist()))\n",
    "    feat_dict = {i + 1: e for i, e in enumerate(unique_vals)}\n",
    "    feat_dict_reverse = {v: k for k, v in feat_dict.items()}\n",
    "\n",
    "    train_feat = train[feature].apply(lambda x: feat_dict_reverse[x]).values\n",
    "    test_feat = test[feature].apply(lambda x: feat_dict_reverse[x]).values\n",
    "\n",
    "    return train_feat, test_feat, feat_dict, feat_dict_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.add_adapter(\"covid_qa\") #, config=adapter_config)\n",
    "#model.train_adapter(\"covid_qa\")\n",
    "#tokenized_covid_dataset = covid_dataset.map(preprocess_function, batched=True, remove_columns=covid_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_args = TrainingArguments(\\n    output_dir=\"./results\",\\n    overwrite_output_dir=True,\\n    evaluation_strategy=\"epoch\",\\n    learning_rate=2e-5,\\n    per_device_train_batch_size=4,\\n    per_device_eval_batch_size=4,\\n    num_train_epochs=3,\\n    weight_decay=0.01,\\n    remove_unused_columns=False,\\n)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=dataset,\\n    eval_dataset=dataset,\\n    tokenizer=tokenizer,\\n    data_collator=data_collator,\\n)'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\"\"\"\n",
    "\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cpu(x):\n",
    "    return x.contiguous().detach().cpu()\n",
    "\n",
    "\n",
    "def to_numpy(x):\n",
    "    return to_cpu(x).numpy()\n",
    "\n",
    "\n",
    "def to_device(xs, device):\n",
    "    if isinstance(xs, tuple) or isinstance(xs, list):\n",
    "        return [x.to(device) for x in xs]\n",
    "    else: return [xs.to(device)]\n",
    "    \n",
    "def infer_batch(inputs, model, device, to_numpy=True):\n",
    "    inputs = to_device(inputs, device)\n",
    "    predicted = model(*inputs)\n",
    "    inputs = [x.cpu() for x in inputs]\n",
    "    preds = torch.sigmoid(predicted)\n",
    "    if to_numpy: preds = preds.cpu().detach().numpy().astype(np.float32)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def infer(model, loader, checkpoint_file=None, device=torch.device('cuda')):\n",
    "    n_obs = len(loader.dataset)\n",
    "    batch_sz = loader.batch_size\n",
    "    predictions = np.zeros((n_obs, N_TARGETS))\n",
    "\n",
    "    if checkpoint_file is not None:\n",
    "        print(f'Starting inference for model: {checkpoint_file}')\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.float()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(tqdm(loader)):\n",
    "            start_index = i * batch_sz\n",
    "            end_index = min(start_index + batch_sz, n_obs)\n",
    "            batch_preds = infer_batch(inputs, model, device)\n",
    "            predictions[start_index:end_index, :] += batch_preds\n",
    "\n",
    "    return predictions\n",
    "def init_seed(seed=100):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "def lin_layer(n_in, n_out, dropout):\n",
    "    return nn.Sequential(nn.Linear(n_in, n_out), GELU(), nn.Dropout(dropout))\n",
    "\n",
    "class Head2(nn.Module):\n",
    "    def __init__(self, n_h=512, n_feats=74, n_bert=768, dropout=0.2):\n",
    "        super().__init__()\n",
    "        n_x = n_feats + 2 * n_bert\n",
    "        self.lin = lin_layer(n_in=n_x, n_out=n_h, dropout=dropout)\n",
    "        self.lin_q = lin_layer(n_in=n_feats + n_bert, n_out=n_h, dropout=dropout)\n",
    "        self.lin_a = lin_layer(n_in=n_feats + n_bert, n_out=n_h, dropout=dropout)\n",
    "        self.head_q = nn.Linear(2 * n_h, N_Q_TARGETS)\n",
    "        self.head_a = nn.Linear(2 * n_h, N_A_TARGETS)\n",
    "\n",
    "    def forward(self, x_feats, x_q_bert, x_a_bert):\n",
    "        x_q = self.lin_q(torch.cat([x_feats, x_q_bert], dim=1))\n",
    "        x_a = self.lin_a(torch.cat([x_feats, x_a_bert], dim=1))\n",
    "        x = self.lin(torch.cat([x_feats, x_q_bert, x_a_bert], dim=1))\n",
    "        x_q = self.head_q(torch.cat([x, x_q], dim=1))\n",
    "        x_a = self.head_a(torch.cat([x, x_a], dim=1))\n",
    "        return torch.cat([x_q, x_a], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPooledXLNet(XLNetModel):\n",
    "    def forward(self, ids, seg_ids=None):\n",
    "        att_mask = (ids > 0).float()\n",
    "        x_bert = super().forward(ids, att_mask, token_type_ids=seg_ids)[0]\n",
    "        att_mask = att_mask.unsqueeze(-1)\n",
    "        return (x_bert * att_mask).sum(dim=1) / att_mask.sum(dim=1)\n",
    "class CustomXLNet(nn.Module):\n",
    "    def __init__(self, n_h, n_feats, head_dropout=0.2):\n",
    "        super().__init__()\n",
    "        #config = XLNetConfig.from_json_file('xlnet-base-cased/config.json') \n",
    "        config = XLNetConfig()#{}\n",
    "        self.xlnet = AvgPooledXLNet(config)\n",
    "        self.head = Head2(n_h, n_feats, n_bert=768, dropout=head_dropout)\n",
    "\n",
    "    def forward(self, x_feats, q_ids, a_ids, seg_q_ids=None, seg_a_ids=None):\n",
    "        x_q_bert = self.xlnet(q_ids, seg_q_ids)\n",
    "        x_a_bert = self.xlnet(a_ids, seg_a_ids)\n",
    "        return self.head(x_feats, x_q_bert, x_a_bert)\n",
    "def get_preds(train, test, ModelClass, tokenizer, model_name, checkpoint_dir, folds):\n",
    "\n",
    "    seg_ids_test, ids_test = {}, {}\n",
    "    max_seq_len = 512\n",
    "    for mode, df in [('test', test)]:\n",
    "        for text, cols in [('question', ['question_title', 'question_body']), \n",
    "                            ('answer', ['question_title', 'answer'])]:\n",
    "            ids, seg_ids = [], []\n",
    "            for x1, x2 in tqdm(df[cols].values):\n",
    "                encoded_inputs = tokenizer.encode_plus(\n",
    "                    x1, x2, add_special_tokens=True, max_length=max_seq_len, truncation =True, padding = 'max_length', # pad_to_max_length=True,  #\n",
    "                    return_token_type_ids=True\n",
    "                )\n",
    "                ids.append(encoded_inputs['input_ids'])\n",
    "                seg_ids.append(encoded_inputs['token_type_ids'])\n",
    "            ids_test[text] = np.array(ids)\n",
    "            seg_ids_test[text] = np.array(seg_ids)\n",
    "\n",
    "    train_category, test_category, category_dict, category_dict_reverse = \\\n",
    "        get_categorical_features(train, test, 'category')\n",
    "\n",
    "    cat_features_train = train_category.reshape(-1, 1)\n",
    "    cat_features_test = test_category.reshape(-1, 1)\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(cat_features_train)\n",
    "    cat_features_test = ohe.transform(cat_features_test).toarray()\n",
    "\n",
    "    num_workers = 8\n",
    "    device = 'cuda'\n",
    "\n",
    "    bs_test = 2\n",
    "    test_loader = DataLoader(\n",
    "        TextDataset5(cat_features_test, ids_test['question'], ids_test['answer'], \n",
    "                        seg_ids_test['question'], seg_ids_test['answer'], test.index),\n",
    "        batch_size=bs_test, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    init_seed()\n",
    "    preds = np.zeros((len(test), N_TARGETS))\n",
    "    for fold_id in folds:\n",
    "        checkpoint_file = f'{checkpoint_dir}{model_name}_fold_{fold_id + 1}_best.pth'\n",
    "        model = ModelClass(256, cat_features_test.shape[1]).to(device)\n",
    "        test_preds = infer(model, test_loader, checkpoint_file, device)\n",
    "        preds += test_preds / len(folds)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def get_xlnet_preds(train, test):\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    model_name = 'siamese_xlnet_1_comb'\n",
    "    checkpoint_dir = 'xlnet-model/'\n",
    "    return get_preds(train, test, CustomXLNet, tokenizer, model_name, checkpoint_dir, [0, 1, 2, 4, 5, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebad25ac01e4f1ab83e9c6ded493878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ffafa8fe86471cadd0bbd72573b56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 5.23 GiB already allocated; 0 bytes free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\xlnet_approach.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000011?line=1'>2</a>\u001b[0m transformers\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39mset_verbosity_error()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000011?line=2'>3</a>\u001b[0m xlnet_pred \u001b[39m=\u001b[39m get_xlnet_preds(train_df, test_df)\n",
      "\u001b[1;32mc:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\xlnet_approach.ipynb Cell 8'\u001b[0m in \u001b[0;36mget_xlnet_preds\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000005?line=67'>68</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msiamese_xlnet_1_comb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000005?line=68'>69</a>\u001b[0m checkpoint_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mxlnet-model/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000005?line=69'>70</a>\u001b[0m \u001b[39mreturn\u001b[39;00m get_preds(train, test, CustomXLNet, tokenizer, model_name, checkpoint_dir, [\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m7\u001b[39;49m, \u001b[39m8\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\xlnet_approach.ipynb Cell 8'\u001b[0m in \u001b[0;36mget_preds\u001b[1;34m(train, test, ModelClass, tokenizer, model_name, checkpoint_dir, folds)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000005?line=57'>58</a>\u001b[0m \u001b[39mfor\u001b[39;00m fold_id \u001b[39min\u001b[39;00m folds:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000005?line=58'>59</a>\u001b[0m     checkpoint_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_dir\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_fold_\u001b[39m\u001b[39m{\u001b[39;00mfold_id \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_best.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000005?line=59'>60</a>\u001b[0m     model \u001b[39m=\u001b[39m ModelClass(\u001b[39m256\u001b[39;49m, cat_features_test\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000005?line=60'>61</a>\u001b[0m     test_preds \u001b[39m=\u001b[39m infer(model, test_loader, checkpoint_file, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/xlnet_approach.ipynb#ch0000005?line=61'>62</a>\u001b[0m     preds \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m test_preds \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(folds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=906'>907</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 578 (2 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=596'>597</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=901'>902</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 5.23 GiB already allocated; 0 bytes free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "xlnet_pred = get_xlnet_preds(train_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac9e7467ab50678fb25b34fbfebaa7dd0935f663e602be01974fdf6c9ce75ada"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
