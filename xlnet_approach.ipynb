{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\km201\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchaudio\\backend\\utils.py:66: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "import os\n",
    "#%pip install scipy\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import *\n",
    "\n",
    "N_TARGETS = 30\n",
    "N_Q_TARGETS = 21\n",
    "N_A_TARGETS = 9\n",
    "global TARGETS \n",
    "TARGETS = [\n",
    "    'question_asker_intent_understanding', 'question_body_critical',\n",
    "    'question_conversational', 'question_expect_short_answer',\n",
    "    'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "    'question_interestingness_others', 'question_interestingness_self',\n",
    "    'question_multi_intent', 'question_not_really_a_question',\n",
    "    'question_opinion_seeking', 'question_type_choice',\n",
    "    'question_type_compare', 'question_type_consequence',\n",
    "    'question_type_definition', 'question_type_entity',\n",
    "    'question_type_instructions', 'question_type_procedure',\n",
    "    'question_type_reason_explanation', 'question_type_spelling',\n",
    "    'question_well_written', 'answer_helpful',\n",
    "    'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "    'answer_satisfaction', 'answer_type_instructions',\n",
    "    'answer_type_procedure', 'answer_type_reason_explanation',\n",
    "    'answer_well_written'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "#data import \n",
    "train_df = pd.read_csv('data/train.csv').fillna(' ')\n",
    "test_df = pd.read_csv('data/test.csv').fillna(' ')\n",
    "#display (train_df)\n",
    "print (os.listdir('data'))\n",
    "#dataset = Dataset.from_pandas(train_df)\n",
    "#data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
    "#dataset = load_dataset(\"data\", data_files=data_files)\n",
    "#print (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, DefaultDataCollator, XLNetConfig, XLNetLMHeadModel, XLNetModel\n",
    "model_name = 'xlnet-base-cased' # 'xlnet-large-cased', 'tiny-xlnet-base-cased', 'jkgrad/xlnet-base-squadv2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset5(Dataset):\n",
    "\n",
    "    def __init__(self, x_features, question_ids, answer_ids, seg_question_ids, \n",
    "                    seg_answer_ids, idxs, targets=None):\n",
    "        self.question_ids = question_ids[idxs].astype(np.int64) #np.long\n",
    "        self.answer_ids = answer_ids[idxs].astype(np.int64)\n",
    "        self.seg_question_ids = seg_question_ids[idxs].astype(np.int64)\n",
    "        self.seg_answer_ids = seg_answer_ids[idxs].astype(np.int64)\n",
    "        self.x_features = x_features[idxs].astype(np.float32)\n",
    "        if targets is not None: self.targets = targets[idxs].astype(np.float32)\n",
    "        else: self.targets = np.zeros((self.x_features.shape[0], N_TARGETS), dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_ids = self.question_ids[idx]\n",
    "        a_ids = self.answer_ids[idx]\n",
    "        seg_q_ids = self.seg_question_ids[idx]\n",
    "        seg_a_ids = self.seg_answer_ids[idx]\n",
    "        x_feats = self.x_features[idx]\n",
    "        target = self.targets[idx]\n",
    "        return (x_feats, q_ids, a_ids, seg_q_ids, seg_a_ids), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_features)\n",
    "#features\n",
    "def get_categorical_features(train, test, feature):\n",
    "    unique_vals = list(set(train[feature].unique().tolist() \n",
    "                            + test[feature].unique().tolist()))\n",
    "    feat_dict = {i + 1: e for i, e in enumerate(unique_vals)}\n",
    "    feat_dict_reverse = {v: k for k, v in feat_dict.items()}\n",
    "\n",
    "    train_feat = train[feature].apply(lambda x: feat_dict_reverse[x]).values\n",
    "    test_feat = test[feature].apply(lambda x: feat_dict_reverse[x]).values\n",
    "\n",
    "    return train_feat, test_feat, feat_dict, feat_dict_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cpu(x):\n",
    "    return x.contiguous().detach().cpu()\n",
    "\n",
    "\n",
    "def to_numpy(x):\n",
    "    return to_cpu(x).numpy()\n",
    "\n",
    "\n",
    "def to_device(xs, device):\n",
    "    if isinstance(xs, tuple) or isinstance(xs, list):\n",
    "        return [x.to(device) for x in xs]\n",
    "    else: return [xs.to(device)]\n",
    "    \n",
    "def infer_batch(inputs, model, device, to_numpy=True):\n",
    "    inputs = to_device(inputs, device)\n",
    "    predicted = model(*inputs)\n",
    "    inputs = [x.cpu() for x in inputs]\n",
    "    preds = torch.sigmoid(predicted)\n",
    "    if to_numpy: preds = preds.cpu().detach().numpy().astype(np.float32)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def infer(model, loader, checkpoint_file=None, device=torch.device('cuda')):\n",
    "    n_obs = len(loader.dataset)\n",
    "    batch_sz = loader.batch_size\n",
    "    predictions = np.zeros((n_obs, N_TARGETS))\n",
    "\n",
    "    if checkpoint_file is not None:\n",
    "        print(f'Starting inference for model: {checkpoint_file}')\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.float()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(tqdm(loader)):\n",
    "            start_index = i * batch_sz\n",
    "            end_index = min(start_index + batch_sz, n_obs)\n",
    "            batch_preds = infer_batch(inputs, model, device)\n",
    "            predictions[start_index:end_index, :] += batch_preds\n",
    "\n",
    "    return predictions\n",
    "def init_seed(seed=100):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "def lin_layer(n_in, n_out, dropout):\n",
    "    return nn.Sequential(nn.Linear(n_in, n_out), GELU(), nn.Dropout(dropout))\n",
    "\n",
    "class Head2(nn.Module):\n",
    "    def __init__(self, n_h=512, n_feats=74, n_bert=768, dropout=0.2):\n",
    "        super().__init__()\n",
    "        n_x = n_feats + 2 * n_bert\n",
    "        self.lin = lin_layer(n_in=n_x, n_out=n_h, dropout=dropout)\n",
    "        self.lin_q = lin_layer(n_in=n_feats + n_bert, n_out=n_h, dropout=dropout)\n",
    "        self.lin_a = lin_layer(n_in=n_feats + n_bert, n_out=n_h, dropout=dropout)\n",
    "        self.head_q = nn.Linear(2 * n_h, N_Q_TARGETS)\n",
    "        self.head_a = nn.Linear(2 * n_h, N_A_TARGETS)\n",
    "\n",
    "    def forward(self, x_feats, x_q_bert, x_a_bert):\n",
    "        x_q = self.lin_q(torch.cat([x_feats, x_q_bert], dim=1))\n",
    "        x_a = self.lin_a(torch.cat([x_feats, x_a_bert], dim=1))\n",
    "        x = self.lin(torch.cat([x_feats, x_q_bert, x_a_bert], dim=1))\n",
    "        x_q = self.head_q(torch.cat([x, x_q], dim=1))\n",
    "        x_a = self.head_a(torch.cat([x, x_a], dim=1))\n",
    "        return torch.cat([x_q, x_a], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPooledXLNet(XLNetModel):\n",
    "    def forward(self, ids, seg_ids=None):\n",
    "        att_mask = (ids > 0).float()\n",
    "        x_bert = super().forward(ids, att_mask, token_type_ids=seg_ids)[0]\n",
    "        att_mask = att_mask.unsqueeze(-1)\n",
    "        return (x_bert * att_mask).sum(dim=1) / att_mask.sum(dim=1)\n",
    "class CustomXLNet(nn.Module):\n",
    "    def __init__(self, n_h, n_feats, head_dropout=0.2):\n",
    "        super().__init__()\n",
    "        #config = XLNetConfig.from_json_file('xlnet-base-cased/config.json') \n",
    "        config = XLNetConfig(d_inner=3072, d_model=768, n_head=12, n_layer=12)#using same format as example\n",
    "        self.xlnet = AvgPooledXLNet(config)\n",
    "        self.head = Head2(n_h, n_feats, n_bert=768, dropout=head_dropout)\n",
    "\n",
    "    def forward(self, x_feats, q_ids, a_ids, seg_q_ids=None, seg_a_ids=None):\n",
    "        x_q_bert = self.xlnet(q_ids, seg_q_ids)\n",
    "        x_a_bert = self.xlnet(a_ids, seg_a_ids)\n",
    "        return self.head(x_feats, x_q_bert, x_a_bert)\n",
    "def get_preds(train, test, ModelClass, tokenizer, model_name, checkpoint_dir, folds):\n",
    "\n",
    "    seg_ids_test, ids_test = {}, {}\n",
    "    max_seq_len = 512\n",
    "    for mode, df in [('test', test)]:\n",
    "        for text, cols in [('question', ['question_title', 'question_body']), \n",
    "                            ('answer', ['question_title', 'answer'])]:\n",
    "            ids, seg_ids = [], []\n",
    "            for x1, x2 in tqdm(df[cols].values):\n",
    "                encoded_inputs = tokenizer.encode_plus(\n",
    "                    x1, x2, add_special_tokens=True, max_length=max_seq_len, truncation =True, padding = 'max_length', # pad_to_max_length=True,  #\n",
    "                    return_token_type_ids=True\n",
    "                )\n",
    "                ids.append(encoded_inputs['input_ids'])\n",
    "                seg_ids.append(encoded_inputs['token_type_ids'])\n",
    "            ids_test[text] = np.array(ids)\n",
    "            seg_ids_test[text] = np.array(seg_ids)\n",
    "\n",
    "    train_category, test_category, category_dict, category_dict_reverse = \\\n",
    "        get_categorical_features(train, test, 'category')\n",
    "\n",
    "    cat_features_train = train_category.reshape(-1, 1)\n",
    "    cat_features_test = test_category.reshape(-1, 1)\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(cat_features_train)\n",
    "    cat_features_test = ohe.transform(cat_features_test).toarray()\n",
    "\n",
    "    num_workers = 8\n",
    "    device = 'cuda'\n",
    "\n",
    "    bs_test = 2\n",
    "    test_loader = DataLoader(\n",
    "        TextDataset5(cat_features_test, ids_test['question'], ids_test['answer'], \n",
    "                        seg_ids_test['question'], seg_ids_test['answer'], test.index),\n",
    "        batch_size=bs_test, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    init_seed()\n",
    "    preds = np.zeros((len(test), N_TARGETS))\n",
    "    for fold_id in folds:\n",
    "        checkpoint_file = f'{checkpoint_dir}{model_name}_fold_{fold_id + 1}_best.pth'\n",
    "        model = ModelClass(256, cat_features_test.shape[1]).to(device)\n",
    "        test_preds = infer(model, test_loader, checkpoint_file, device)\n",
    "        preds += test_preds / len(folds)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def get_xlnet_preds(train, test):\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "    model_name = 'siamese_xlnet_1_comb'\n",
    "    checkpoint_dir = 'xlnet-model/'\n",
    "    return get_preds(train, test, CustomXLNet, tokenizer, model_name, checkpoint_dir, [0, 1, 2, 4, 5, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr(trues, preds, n_bins=None):\n",
    "    rhos = []\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        if len(np.unique(col_pred)) == 1:\n",
    "            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n",
    "        rhos.append(spearmanr(col_trues, col_pred).correlation)\n",
    "    return np.mean(rhos)\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(outputs, targets, alpha=0.5, margin=0.1, question_only=False):\n",
    "    if question_only:\n",
    "        outputs = outputs[:, :21]\n",
    "        targets = targets[:, :21]\n",
    "    bce = F.binary_cross_entropy_with_logits(outputs, targets, reduction=\"none\")\n",
    "    bce = (bce * LABEL_WEIGHTS[:bce.size(-1)]).mean()\n",
    "    \n",
    "    batch_size = outputs.size(0)\n",
    "    if batch_size % 2 == 0:\n",
    "        outputs1, outputs2 = outputs.sigmoid().contiguous().view(2, batch_size // 2, outputs.size(-1))\n",
    "        targets1, targets2 = targets.contiguous().view(2, batch_size // 2, outputs.size(-1))\n",
    "        # 1 if first ones are larger, -1 if second ones are larger, and 0 if equals.\n",
    "        ordering = (targets1 > targets2).float() - (targets1 < targets2).float()\n",
    "        margin_rank_loss = (-ordering * (outputs1 - outputs2) + margin).clamp(min=0.0)\n",
    "        margin_rank_loss = (margin_rank_loss * LABEL_WEIGHTS[:outputs.size(-1)]).mean()\n",
    "    else:\n",
    "        # batch size is not even number, so we can't devide them into pairs.\n",
    "        margin_rank_loss = 0.0\n",
    "\n",
    "    return alpha * bce + (1 - alpha) * margin_rank_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output categories:\n",
      "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "input categories:\n",
      "\t ['question_title', 'question_body', 'answer']\n",
      "cuda\n",
      "question_asker_intent_understanding \t 0.9666648\n",
      "question_body_critical \t 0.58160704\n",
      "question_conversational \t 0.7005903\n",
      "question_expect_short_answer \t 0.36372498\n",
      "question_fact_seeking \t 0.4212381\n",
      "question_has_commonly_accepted_answer \t 0.3791943\n",
      "question_interestingness_others \t 0.9392594\n",
      "question_interestingness_self \t 0.6863097\n",
      "question_multi_intent \t 0.3809655\n",
      "question_not_really_a_question \t 2.7880754\n",
      "question_opinion_seeking \t 0.34880248\n",
      "question_type_choice \t 0.346085\n",
      "question_type_compare \t 0.8308305\n",
      "question_type_consequence \t 1.7193657\n",
      "question_type_definition \t 0.92452765\n",
      "question_type_entity \t 0.646035\n",
      "question_type_instructions \t 0.3016625\n",
      "question_type_procedure \t 0.4960922\n",
      "question_type_reason_explanation \t 0.33294326\n",
      "question_type_spelling \t 6.230046\n",
      "question_well_written \t 0.7154175\n",
      "answer_helpful \t 1.1115448\n",
      "answer_level_of_information \t 1.1855657\n",
      "answer_plausible \t 1.4684314\n",
      "answer_relevance \t 1.7103598\n",
      "answer_satisfaction \t 0.97630584\n",
      "answer_type_instructions \t 0.3018177\n",
      "answer_type_procedure \t 0.56550634\n",
      "answer_type_reason_explanation \t 0.3135491\n",
      "answer_well_written \t 1.2674823\n"
     ]
    }
   ],
   "source": [
    "output_categories = list(train_df.columns[11:])\n",
    "input_categories = list(train_df.columns[[1,2,5]])\n",
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "LABEL_WEIGHTS = torch.tensor(1.0 / train_df[output_categories].std().values, dtype=torch.float32).to(device)\n",
    "LABEL_WEIGHTS = LABEL_WEIGHTS / LABEL_WEIGHTS.sum() * 30\n",
    "for name, weight in zip(output_categories, LABEL_WEIGHTS.cpu().numpy()):\n",
    "    print(name, \"\\t\", weight)\n",
    "\n",
    "def load_prep(test_df, tokenizer):\n",
    "    seg_ids_test, ids_test = {}, {}\n",
    "    max_seq_len = 512\n",
    "    for mode, df in [('test', test_df)]:\n",
    "        for text, cols in [('question', ['question_title', 'question_body']), \n",
    "                            ('answer', ['question_title', 'answer'])]:\n",
    "            ids, seg_ids = [], []\n",
    "            for x1, x2 in tqdm(df[cols].values):\n",
    "                encoded_inputs = tokenizer.encode_plus(\n",
    "                    x1, x2, add_special_tokens=True, max_length=max_seq_len, truncation =True, padding = 'max_length', # pad_to_max_length=True,  #\n",
    "                    return_token_type_ids=True\n",
    "                )\n",
    "                ids.append(encoded_inputs['input_ids'])\n",
    "                seg_ids.append(encoded_inputs['token_type_ids'])\n",
    "            ids_test[text] = np.array(ids)\n",
    "            seg_ids_test[text] = np.array(seg_ids)\n",
    "    return seg_ids_test, ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_dict = torch.load('xlnet-model/siamese_xlnet_1_comb_fold_1_best.pth')\n",
    "#print (torch_dict['model_state_dict'].keys())\n",
    "#trainer\n",
    "def train_and_predict(train_data, test_data, epochs, batch_size):\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "    #dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    #test_dataloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    #integration into format for \n",
    "    num_workers = 8\n",
    "    seg_ids_train, ids_train = load_prep(train_data, tokenizer)\n",
    "    seg_ids_test, ids_test = load_prep(test_data, tokenizer)\n",
    "    train_category, test_category, category_dict, category_dict_reverse = \\\n",
    "        get_categorical_features(train_data, test_data, 'category')\n",
    "    cat_features_train = train_category.reshape(-1, 1)\n",
    "    cat_features_test = test_category.reshape(-1, 1)\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(cat_features_train)\n",
    "    cat_features_train = ohe.transform(cat_features_train).toarray()\n",
    "    cat_features_test = ohe.transform(cat_features_test).toarray()\n",
    "    print (len(ids_train['question'][0]))\n",
    "    print (len(cat_features_train))\n",
    "    train_df_targets = train_df[TARGETS].to_numpy()\n",
    "   \n",
    "    dataloader = DataLoader(\n",
    "        TextDataset5(cat_features_train, ids_train['question'], ids_train['answer'], \n",
    "                        seg_ids_train['question'], seg_ids_train['answer'], train_df.index, targets = train_df_targets), #include targets\n",
    "        batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        TextDataset5(cat_features_test, ids_test['question'], ids_test['answer'], \n",
    "                        seg_ids_test['question'], seg_ids_test['answer'], test_df.index),\n",
    "        batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    ) # also needs a train_loader\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "\n",
    "    model = CustomXLNet(256, cat_features_test.shape[1]).to(device)\n",
    "    #model = Model().to(device) #change this\n",
    "    test_predictions = []\n",
    "\n",
    "    ## Q and A\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    #for thing in model.parameters():\n",
    "    #    print (type(thing))\n",
    "    #print ([p for n, p in model.named_parameters() if p.requires_grad and \"xlnet\" not in n])\n",
    "    params = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"xlnet\" in n],\n",
    "            \"weight_decay\": 1e-2,\n",
    "            \"lr\": 5e-5\n",
    "        }\n",
    "    ]\n",
    "    optimizer = AdamW(params)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(len(dataloader) * (epochs) * 0.05),\n",
    "        num_training_steps=len(dataloader) * (epochs)\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        import time\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "        for x_feats, q_ids, a_ids, seg_q_ids, seg_a_ids in tqdm(dataloader, total=len(dataloader)): \n",
    "            #input_ids, token_type_ids, attention_mask, targets #all of this stuff needs changes\n",
    "            #input_ids = input_ids.to(device)\n",
    "            #token_type_ids = token_type_ids.to(device)\n",
    "            #attention_mask = attention_mask.to(device)\n",
    "            x_feats = x_feats.to(device)\n",
    "            q_ids = q_ids.to(device)\n",
    "            a_ids = a_ids.to(device)\n",
    "            seg_q_ids = seg_q_ids.to(device)\n",
    "            seg_a_ids = seg_a_ids.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(x_feats, q_ids, a_ids, seg_q_ids, seg_a_ids)\n",
    "            #outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids) #fix model part\n",
    "            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n",
    "            train_targets.extend(targets.detach().cpu().numpy())\n",
    "            loss = compute_loss(outputs, targets)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_losses.append(loss.detach().cpu().item())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_preds = []\n",
    "            for x_feats, q_ids, a_ids, seg_q_ids, seg_a_ids in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "                x_feats = x_feats.to(device)\n",
    "                q_ids = q_ids.to(device)\n",
    "                a_ids = a_ids.to(device)\n",
    "                seg_q_ids = seg_q_ids.to(device)\n",
    "                seg_a_ids = seg_a_ids.to(device)\n",
    "                outputs = model(x_feats, q_ids, a_ids, seg_q_ids, seg_a_ids)\n",
    "                test_preds.extend(outputs.sigmoid().cpu().numpy())\n",
    "            test_predictions.append(np.stack(test_preds))\n",
    "            print()\n",
    "        print(\"Epoch {}: Train Loss {}\".format(epoch + 1, np.mean(train_losses)))\n",
    "        print(\"\\t Train Spearmanr {:.4f}\".format(\n",
    "            compute_spearmanr(np.stack(train_targets), np.stack(train_preds))\n",
    "        ))\n",
    "        print(\"\\t elapsed: {}s\".format(time.time() - start))\n",
    "\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d98736e5c0f47b680bc2ae1b7e412d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839f0fab95c9483092dc00ae3ec240dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6079 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8409cd0e9f64ff09bc609303201948f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910abe58932d4e708022ec36b05e16f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/476 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "6079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\km201\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513adb5b366c452abd0558cb5061a325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "trainer_preds = train_and_predict(train_df, test_df, epochs = 2, batch_size=2)\n",
    "\n",
    "#xlnet_pred = get_xlnet_preds(train_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac9e7467ab50678fb25b34fbfebaa7dd0935f663e602be01974fdf6c9ce75ada"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
