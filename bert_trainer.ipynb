{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.9.1\n","1.11.0+cu113\n"]}],"source":["#%pip install tensorflow\n","from typing import List, Tuple\n","import random\n","import html\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import GroupKFold, KFold\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","import os\n","from scipy.stats import spearmanr\n","from scipy.optimize import minimize\n","from math import floor, ceil\n","from transformers import *\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def tqdm(it, *args, **kwargs):\n","    return it\n","\n","\n","def seed_everything(seed=1234):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","seed_everything()\n","np.set_printoptions(suppress=True)\n","\n","print(tf.__version__)\n","print(torch.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Read data and tokenizer"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Didn't find file bert-model\\added_tokens.json. We won't load it.\n","Didn't find file bert-model\\special_tokens_map.json. We won't load it.\n","Didn't find file bert-model\\tokenizer_config.json. We won't load it.\n","loading file bert-model\\vocab.txt\n","loading file None\n","loading file None\n","loading file None\n"]},{"name":"stdout","output_type":"stream","text":["['sample_submission.csv', 'test.csv', 'train.csv']\n","train shape = (6079, 41)\n","test shape = (476, 11)\n","\n","output categories:\n","\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n","\n","input categories:\n","\t ['question_title', 'question_body', 'answer']\n"]}],"source":["PATH = 'data/'\n","\n","BERT_PATH = 'bert-model'\n","\n","#tokenizer = BertTokenizer.from_pretrained('bert-model') #'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","MAX_SEQUENCE_LENGTH = 512\n","print (os.listdir('data'))\n","df_train = pd.read_csv(PATH+'train.csv')\n","df_test = pd.read_csv(PATH+'test.csv')\n","sub = pd.read_csv(PATH+'sample_submission.csv')\n","print('train shape =', df_train.shape)\n","print('test shape =', df_test.shape)\n","\n","output_categories = list(df_train.columns[11:])\n","input_categories = list(df_train.columns[[1,2,5]])\n","print('\\noutput categories:\\n\\t', output_categories)\n","print('\\ninput categories:\\n\\t', input_categories)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Preprocessing"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["df_train.question_body = df_train.question_body.apply(html.unescape)\n","df_train.question_title = df_train.question_title.apply(html.unescape)\n","df_train.answer = df_train.answer.apply(html.unescape)\n","\n","df_test.question_body = df_test.question_body.apply(html.unescape)\n","df_test.question_title = df_test.question_title.apply(html.unescape)\n","df_test.answer = df_test.answer.apply(html.unescape)"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["([101, 2054, 2572, 1045, 3974, 2043, 2478, 5331, 10868, 2612, 1997, 1037, 26632, 10014, 1029, 102, 2044, 2652, 2105, 2007, 26632, 5855, 2006, 1011, 1996, 1011, 10036, 1006, 3191, 1024, 11674, 10014, 1010, 7065, 1012, 10014, 5614, 2006, 1037, 3442, 10014, 1010, 13135, 5331, 10868, 1007, 1010, 1045, 2052, 2066, 2000, 2131, 2582, 2007, 2023, 1012, 1996, 3471, 2007, 1996, 5461, 1045, 2109, 2003, 2008, 3579, 2003, 6410, 1998, 18892, 2491, 2003, 18636, 2012, 2190, 1012, 2023, 3132, 2026, 16437, 2000, 2145, 5739, 1006, 3191, 1024, 2757, 9728, 1007, 2085, 1010, 2004, 3500, 2003, 8455, 1010, 1045, 2215, 2000, 2022, 2583, 2000, 5607, 2444, 9728, 1012, 1045, 2903, 2008, 2005, 2023, 1010, 8285, 14876, 7874, 1998, 2275, 10880, 18892, 2097, 2022, 1997, 2307, 2393, 1012, 2061, 1010, 2028, 5793, 2021, 6450, 5724, 2003, 1037, 26632, 10014, 1006, 2360, 1010, 1041, 2546, 2531, 7382, 26632, 1007, 2174, 1010, 1045, 2572, 2025, 2428, 4699, 1999, 2664, 2178, 3539, 10014, 1012, 2019, 4522, 2003, 1996, 5992, 5331, 10868, 1012, 3272, 2005, 4555, 7995, 3292, 1010, 2054, 2572, 1045, 3974, 2043, 2478, 10868, 1006, 11211, 2007, 1037, 2986, 10014, 1010, 2360, 1041, 2546, 19841, 1011, 3263, 1013, 1016, 1012, 1022, 1007, 2612, 1997, 1037, 26632, 10014, 1029, 102, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","['[CLS]', 'what', 'am', 'i', 'losing', 'when', 'using', 'extension', 'tubes', 'instead', 'of', 'a', 'macro', 'lens', '?', '[SEP]', 'after', 'playing', 'around', 'with', 'macro', 'photography', 'on', '-', 'the', '-', 'cheap', '(', 'read', ':', 'reversed', 'lens', ',', 'rev', '.', 'lens', 'mounted', 'on', 'a', 'straight', 'lens', ',', 'passive', 'extension', 'tubes', ')', ',', 'i', 'would', 'like', 'to', 'get', 'further', 'with', 'this', '.', 'the', 'problems', 'with', 'the', 'techniques', 'i', 'used', 'is', 'that', 'focus', 'is', 'manual', 'and', 'aperture', 'control', 'is', 'problematic', 'at', 'best', '.', 'this', 'limited', 'my', 'setup', 'to', 'still', 'subjects', '(', 'read', ':', 'dead', 'insects', ')', 'now', ',', 'as', 'spring', 'is', 'approaching', ',', 'i', 'want', 'to', 'be', 'able', 'to', 'shoot', 'live', 'insects', '.', 'i', 'believe', 'that', 'for', 'this', ',', 'auto', '##fo', '##cus', 'and', 'set', '##table', 'aperture', 'will', 'be', 'of', 'great', 'help', '.', 'so', ',', 'one', 'obvious', 'but', 'expensive', 'option', 'is', 'a', 'macro', 'lens', '(', 'say', ',', 'e', '##f', '100', '##mm', 'macro', ')', 'however', ',', 'i', 'am', 'not', 'really', 'interested', 'in', 'yet', 'another', 'prime', 'lens', '.', 'an', 'alternative', 'is', 'the', 'electrical', 'extension', 'tubes', '.', 'except', 'for', 'maximum', 'focusing', 'distance', ',', 'what', 'am', 'i', 'losing', 'when', 'using', 'tubes', '(', 'coupled', 'with', 'a', 'fine', 'lens', ',', 'say', 'e', '##f', '##70', '-', '200', '/', '2', '.', '8', ')', 'instead', 'of', 'a', 'macro', 'lens', '?', '[SEP]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"]}],"source":["def _preprocess_text(s: str) -> str:\n","    return s\n","\n","\n","def _trim_input(question_tokens: List[str], answer_tokens: List[str], max_sequence_length: int, q_max_len: int, a_max_len: int) -> Tuple[List[str], List[str]]:\n","    q_len = len(question_tokens)\n","    a_len = len(answer_tokens)\n","    if q_len + a_len + 3 > max_sequence_length:\n","        if a_max_len <= a_len and q_max_len <= q_len:\n","            ## Answer も Question も長過ぎる場合、どちらも限界まで切り詰めるしかない\n","            q_new_len_head = floor((q_max_len - q_max_len/2))\n","            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n","            a_new_len_head = floor((a_max_len - a_max_len/2))\n","            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n","        elif q_len <= a_len and q_len < q_max_len:\n","            ## Answer のほうが長く、Question が十分短いなら、その分 Answer にまわす\n","            a_max_len = a_max_len + (q_max_len - q_len - 1)\n","            a_new_len_head = floor((a_max_len - a_max_len/2))\n","            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n","        elif a_len < q_len:\n","            assert a_len <= a_max_len\n","            q_max_len = q_max_len + (a_max_len - a_len - 1)\n","            q_new_len_head = floor((q_max_len - q_max_len/2))\n","            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n","        else:\n","            raise ValueError(\"unreachable: q_len: {}, a_len: {}, q_max_len: {}, a_max_len: {}\".format(q_len, a_len, q_max_len, a_max_len))\n","    return question_tokens, answer_tokens\n","\n","\n","def _convert_to_transformer_inputs(title: str, question: str, answer: str, tokenizer: BertTokenizer, question_only=False):\n","    title = _preprocess_text(title)\n","    question = _preprocess_text(question)\n","    answer = _preprocess_text(answer)\n","    question = \"{} [SEP] {}\".format(title, question)\n","    question_tokens = tokenizer.tokenize(question)\n","    if question_only:\n","        answer_tokens = []\n","    else:\n","        answer_tokens = tokenizer.tokenize(answer)\n","    question_tokens, answer_tokens = _trim_input(question_tokens, answer_tokens, MAX_SEQUENCE_LENGTH, (MAX_SEQUENCE_LENGTH - 3) // 2, (MAX_SEQUENCE_LENGTH - 3) // 2)\n","    ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + question_tokens + [\"[SEP]\"] + answer_tokens + [\"[SEP]\"])\n","    padded_ids = ids + [tokenizer.pad_token_id] * (MAX_SEQUENCE_LENGTH - len(ids))\n","    token_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(answer_tokens) + 1) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n","    attention_mask = [1] * len(ids) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n","    return padded_ids, token_type_ids, attention_mask\n","\n","sample_args = df_train[\"question_title\"].values[0], df_train[\"question_body\"].values[0], df_train[\"answer\"].values[0]\n","sample_ids = _convert_to_transformer_inputs(*sample_args, tokenizer, question_only=True)\n","print(sample_ids)\n","print(tokenizer.convert_ids_to_tokens(sample_ids[0]))"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["def compute_input_arrays(df, question_only=False):\n","    input_ids, input_token_type_ids, input_attention_masks = [], [], []\n","    for title, body, answer in zip(df[\"question_title\"].values, df[\"question_body\"].values, df[\"answer\"].values):\n","        ids, type_ids, mask = _convert_to_transformer_inputs(title, body, answer, tokenizer, question_only=question_only)\n","        input_ids.append(ids)\n","        input_token_type_ids.append(type_ids)\n","        input_attention_masks.append(mask)\n","    return (\n","        np.asarray(input_ids, dtype=np.int32),\n","        np.asarray(input_token_type_ids, dtype=np.int32),\n","        np.asarray(input_attention_masks, dtype=np.int32),\n","    )\n","\n","\n","def compute_output_arrays(df):\n","    return np.asarray(df[output_categories])"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Modeling"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["class Model(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        #config = BertConfig.from_json_file(BERT_PATH + \"/bert_config.json\")\n","        config = BertConfig()\n","        config.output_hidden_states = True\n","        #self.bert = BertForPreTraining.from_pretrained(BERT_PATH + \"/bert_model.ckpt.index\", from_tf=True, config=config).bert\n","        self.bert = BertForPreTraining.from_pretrained('bert_base_uncased', config = config)\n","        self.cls_token_head = nn.Sequential(\n","            nn.Dropout(0.1),\n","            nn.Linear(768 * 4, 768),\n","            nn.ReLU(inplace=True),\n","        )\n","        self.qa_sep_token_head = nn.Sequential(\n","            nn.Dropout(0.1),\n","            nn.Linear(768 * 4, 768),\n","            nn.ReLU(inplace=True),\n","        )\n","        self.linear = nn.Sequential(\n","            nn.Dropout(0.1),\n","            nn.Linear(768 * 2, 30),\n","        )\n","        \n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        question_answer_seps = (torch.sum((token_type_ids == 0) * attention_mask, -1) - 1)\n","\n","#         p_question_answer_dropout = 0.2\n","#         if self.training and random.random() < p_question_answer_dropout:\n","#             if random.random() < 0.5:\n","#                 # mask question\n","#                 attention_mask = attention_mask * (token_type_ids == 1)\n","#             else:\n","#                 # mask answer\n","#                 attention_mask = attention_mask * (token_type_ids == 0)\n","        \n","        _, _, hidden_states = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        hidden_states_cls_embeddings = [x[:, 0] for x in hidden_states[-4:]]\n","        x = torch.cat(hidden_states_cls_embeddings, dim=-1)\n","        x_cls = self.cls_token_head(x)\n","        \n","        # Gather [SEP] hidden states\n","        tmp = torch.arange(0, len(input_ids), dtype=torch.long)\n","        hidden_states_qa_sep_embeddings = [x[tmp, question_answer_seps] for x in hidden_states[-4:]]\n","        x = torch.cat(hidden_states_qa_sep_embeddings, dim=-1)\n","        \n","        x_qa_sep = self.qa_sep_token_head(x)\n","        x = torch.cat([x_cls, x_qa_sep], -1)\n","        x = self.linear(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Training"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["outputs = torch.tensor(compute_output_arrays(df_train), dtype=torch.float)\n","inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train)]\n","question_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train, question_only=True)]\n","test_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test)]\n","test_question_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test, question_only=True)]"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading weights file bert-model/bert_model.ckpt.index\n","Converting TensorFlow checkpoint from c:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\bert-model\\bert_model.ckpt\n","Loading TF weight bert/embeddings/LayerNorm/beta with shape [1024]\n"]},{"ename":"OpError","evalue":"NewRandomAccessFile failed to Create/Open: c:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\bert-model\\bert_model.ckpt.data-00000-of-00001 : Access is denied.\r\n; Input/output error","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:66\u001b[0m, in \u001b[0;36mget_tensor\u001b[1;34m(self, tensor_str)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=64'>65</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=65'>66</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m CheckpointReader\u001b[39m.\u001b[39;49mCheckpointReader_GetTensor(\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=66'>67</a>\u001b[0m       \u001b[39mself\u001b[39;49m, compat\u001b[39m.\u001b[39;49mas_bytes(tensor_str))\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=67'>68</a>\u001b[0m \u001b[39m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=68'>69</a>\u001b[0m \u001b[39m# issue with throwing python exceptions from C++.\u001b[39;00m\n","\u001b[1;31mRuntimeError\u001b[0m: NewRandomAccessFile failed to Create/Open: c:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\bert-model\\bert_model.ckpt.data-00000-of-00001 : Access is denied.\r\n; Input/output error","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mOpError\u001b[0m                                   Traceback (most recent call last)","\u001b[1;32mc:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\23th-place-solusion.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000012?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m n, _ \u001b[39min\u001b[39;00m Model()\u001b[39m.\u001b[39mnamed_parameters():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000012?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(n)\n","\u001b[1;32mc:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\23th-place-solusion.ipynb Cell 9'\u001b[0m in \u001b[0;36mModel.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=3'>4</a>\u001b[0m config \u001b[39m=\u001b[39m BertConfig\u001b[39m.\u001b[39mfrom_json_file(BERT_PATH \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/bert_config.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=4'>5</a>\u001b[0m config\u001b[39m.\u001b[39moutput_hidden_states \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert \u001b[39m=\u001b[39m BertForPreTraining\u001b[39m.\u001b[39;49mfrom_pretrained(BERT_PATH \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/bert_model.ckpt.index\u001b[39;49m\u001b[39m\"\u001b[39;49m, from_tf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, config\u001b[39m=\u001b[39;49mconfig)\u001b[39m.\u001b[39mbert\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=7'>8</a>\u001b[0m     nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.1\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=8'>9</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m768\u001b[39m \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m768\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=9'>10</a>\u001b[0m     nn\u001b[39m.\u001b[39mReLU(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=10'>11</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqa_sep_token_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=12'>13</a>\u001b[0m     nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.1\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=13'>14</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m768\u001b[39m \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m768\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=14'>15</a>\u001b[0m     nn\u001b[39m.\u001b[39mReLU(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/km201/Desktop/NLP244_FINAL/244_Final_Project/23th-place-solusion.ipynb#ch0000008?line=15'>16</a>\u001b[0m )\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\modeling_utils.py:1503\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/modeling_utils.py?line=1499'>1500</a>\u001b[0m \u001b[39mif\u001b[39;00m from_tf:\n\u001b[0;32m   <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/modeling_utils.py?line=1500'>1501</a>\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.index\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/modeling_utils.py?line=1501'>1502</a>\u001b[0m         \u001b[39m# Load from a TensorFlow 1.X checkpoint - provided by original authors\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/modeling_utils.py?line=1502'>1503</a>\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload_tf_weights(model, config, resolved_archive_file[:\u001b[39m-\u001b[39;49m\u001b[39m6\u001b[39;49m])  \u001b[39m# Remove the '.index'\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/modeling_utils.py?line=1503'>1504</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/modeling_utils.py?line=1504'>1505</a>\u001b[0m         \u001b[39m# Load from our TensorFlow 2.0 checkpoints\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/modeling_utils.py?line=1505'>1506</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:119\u001b[0m, in \u001b[0;36mload_tf_weights_in_bert\u001b[1;34m(model, config, tf_checkpoint_path)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/models/bert/modeling_bert.py?line=116'>117</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, shape \u001b[39min\u001b[39;00m init_vars:\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/models/bert/modeling_bert.py?line=117'>118</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading TF weight \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m with shape \u001b[39m\u001b[39m{\u001b[39;00mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/models/bert/modeling_bert.py?line=118'>119</a>\u001b[0m     array \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mload_variable(tf_path, name)\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/models/bert/modeling_bert.py?line=119'>120</a>\u001b[0m     names\u001b[39m.\u001b[39mappend(name)\n\u001b[0;32m    <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/transformers/models/bert/modeling_bert.py?line=120'>121</a>\u001b[0m     arrays\u001b[39m.\u001b[39mappend(array)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py:82\u001b[0m, in \u001b[0;36mload_variable\u001b[1;34m(ckpt_dir_or_file, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/checkpoint_utils.py?line=79'>80</a>\u001b[0m   name \u001b[39m=\u001b[39m name[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/checkpoint_utils.py?line=80'>81</a>\u001b[0m reader \u001b[39m=\u001b[39m load_checkpoint(ckpt_dir_or_file)\n\u001b[1;32m---> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/checkpoint_utils.py?line=81'>82</a>\u001b[0m \u001b[39mreturn\u001b[39;00m reader\u001b[39m.\u001b[39;49mget_tensor(name)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:71\u001b[0m, in \u001b[0;36mget_tensor\u001b[1;34m(self, tensor_str)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=67'>68</a>\u001b[0m \u001b[39m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=68'>69</a>\u001b[0m \u001b[39m# issue with throwing python exceptions from C++.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=69'>70</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=70'>71</a>\u001b[0m   error_translator(e)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:45\u001b[0m, in \u001b[0;36merror_translator\u001b[1;34m(e)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=42'>43</a>\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mInternalError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, error_message)\n\u001b[0;32m     <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=43'>44</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/km201/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=44'>45</a>\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mOpError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, error_message, errors_impl\u001b[39m.\u001b[39mUNKNOWN)\n","\u001b[1;31mOpError\u001b[0m: NewRandomAccessFile failed to Create/Open: c:\\Users\\km201\\Desktop\\NLP244_FINAL\\244_Final_Project\\bert-model\\bert_model.ckpt.data-00000-of-00001 : Access is denied.\r\n; Input/output error"]}],"source":["for n, _ in Model().named_parameters():\n","    print(n)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["LABEL_WEIGHTS = torch.tensor(1.0 / df_train[output_categories].std().values, dtype=torch.float32).to(device)\n","LABEL_WEIGHTS = LABEL_WEIGHTS / LABEL_WEIGHTS.sum() * 30\n","for name, weight in zip(output_categories, LABEL_WEIGHTS.cpu().numpy()):\n","    print(name, \"\\t\", weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BEST_BINS = [400, 400, 15, 100, 400, 7, 1600, 100, 100, 400, 100, 9, 8, 50, 9, 8, 15, 400, 400, 5, 400, 400, 800, 50, 200, 1600, 20, 200, 1600, 1600]\n","\n","def binning_output(preds, n_bins=BEST_BINS):\n","    preds = preds.copy()\n","    for i in range(preds.shape[-1]):\n","        n = n_bins[i]\n","        binned = (preds[:, i] * n).astype(np.int32).astype(np.float32) / n\n","        unique_values, unique_counts = np.unique(binned, return_counts=True)\n","        # 多数派以外が 0.5 % を下回ったら binning をやめる\n","        minor_value_ratio = (unique_counts.sum() - unique_counts.max()) / unique_counts.sum()\n","        if minor_value_ratio < 0.005:\n","            keep = np.argsort(preds[:, i])[::-1][:int(len(preds) * 0.005) + 1]\n","            binned[keep] = preds[keep, i]\n","        preds[:, i] = binned\n","    return preds\n","\n","\n","def compute_spearmanr(trues, preds, n_bins=None):\n","    rhos = []\n","    if n_bins:\n","        preds = binning_output(preds, n_bins)\n","    for col_trues, col_pred in zip(trues.T, preds.T):\n","        if len(np.unique(col_pred)) == 1:\n","            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n","        rhos.append(spearmanr(col_trues, col_pred).correlation)\n","    return np.mean(rhos)\n","\n","\n","\n","def compute_loss(outputs, targets, alpha=0.5, margin=0.1, question_only=False):\n","    if question_only:\n","        outputs = outputs[:, :21]\n","        targets = targets[:, :21]\n","    bce = F.binary_cross_entropy_with_logits(outputs, targets, reduction=\"none\")\n","    bce = (bce * LABEL_WEIGHTS[:bce.size(-1)]).mean()\n","    \n","    batch_size = outputs.size(0)\n","    if batch_size % 2 == 0:\n","        outputs1, outputs2 = outputs.sigmoid().contiguous().view(2, batch_size // 2, outputs.size(-1))\n","        targets1, targets2 = targets.contiguous().view(2, batch_size // 2, outputs.size(-1))\n","        # 1 if first ones are larger, -1 if second ones are larger, and 0 if equals.\n","        ordering = (targets1 > targets2).float() - (targets1 < targets2).float()\n","        margin_rank_loss = (-ordering * (outputs1 - outputs2) + margin).clamp(min=0.0)\n","        margin_rank_loss = (margin_rank_loss * LABEL_WEIGHTS[:outputs.size(-1)]).mean()\n","    else:\n","        # batch size is not even number, so we can't devide them into pairs.\n","        margin_rank_loss = 0.0\n","\n","    return alpha * bce + (1 - alpha) * margin_rank_loss\n","\n","\n","def train_and_predict(train_data, valid_data, test_data, q_train_data, q_valid_data, q_test_data, q_epochs, epochs, batch_size, fold):\n","    dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","    valid_dataloader = torch.utils.data.DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n","    test_dataloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)\n","    q_dataloader = torch.utils.data.DataLoader(q_train_data, shuffle=True, batch_size=batch_size)\n","    q_valid_dataloader = torch.utils.data.DataLoader(q_valid_data, shuffle=False, batch_size=batch_size)\n","    q_test_dataloader = torch.utils.data.DataLoader(q_test_data, shuffle=False, batch_size=batch_size)\n","\n","    model = Model().to(device)\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n","            \"weight_decay\": 1e-2,\n","            \"lr\": 5e-5\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n], \n","            \"weight_decay\": 0.0,\n","            \"lr\": 5e-5\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n","            \"weight_decay\": 1e-2,\n","            \"lr\": 5e-4\n","            \n","        }\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=int(len(dataloader) * (q_epochs) * 0.05),\n","        num_training_steps=len(dataloader) * (q_epochs)\n","    )\n","    \n","    test_predictions = []\n","    valid_predictions = []\n","\n","    ## Question Only\n","    for epoch in range(q_epochs): \n","        import time\n","        start = time.time()\n","        model.train()\n","        train_losses = []\n","        train_preds = []\n","        train_targets = []\n","        for input_ids, token_type_ids, attention_mask, targets in tqdm(q_dataloader, total=len(q_dataloader)):\n","            input_ids = input_ids.to(device)\n","            token_type_ids = token_type_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            targets = targets.to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n","            train_targets.extend(targets.detach().cpu().numpy())\n","            loss = compute_loss(outputs, targets, question_only=True)\n","            model.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            train_losses.append(loss.detach().cpu().item())\n","        model.eval()\n","        valid_losses = []\n","        valid_preds = []\n","        valid_targets = []\n","        with torch.no_grad():\n","            for input_ids, token_type_ids, attention_mask, targets in tqdm(q_valid_dataloader, total=len(q_valid_dataloader)):\n","                input_ids = input_ids.to(device)\n","                token_type_ids = token_type_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","                targets = targets.to(device)\n","                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","                prob = outputs.sigmoid()\n","                prob[:, 21:] = 0.0\n","                valid_preds.extend(prob.cpu().numpy())\n","                valid_targets.extend(targets.cpu().numpy())\n","                loss = compute_loss(outputs, targets, question_only=True)\n","                valid_losses.append(loss.detach().cpu().item())\n","            valid_predictions.append(np.stack(valid_preds))\n","            test_preds = []\n","            for input_ids, token_type_ids, attention_mask in tqdm(q_test_dataloader, total=len(q_test_dataloader)):\n","                input_ids = input_ids.to(device)\n","                token_type_ids = token_type_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","                prob = outputs.sigmoid()\n","                prob[:, 21:] = 0.0\n","                test_preds.extend(prob.cpu().numpy())\n","            test_predictions.append(np.stack(test_preds))\n","            print()\n","        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n","        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n","            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n","            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) / len(valid_predictions)),\n","            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n","        ))\n","        print(\"\\t elapsed: {}s\".format(time.time() - start))\n","\n","    ## Q and A\n","    model = Model().to(device)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n","            \"weight_decay\": 1e-2,\n","            \"lr\": 5e-5\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n], \n","            \"weight_decay\": 0.0,\n","            \"lr\": 5e-5\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n","            \"weight_decay\": 1e-2,\n","            \"lr\": 5e-4\n","            \n","        }\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=int(len(dataloader) * (epochs) * 0.05),\n","        num_training_steps=len(dataloader) * (epochs)\n","    )\n","\n","    for epoch in range(epochs): \n","        import time\n","        start = time.time()\n","        model.train()\n","        train_losses = []\n","        train_preds = []\n","        train_targets = []\n","        for input_ids, token_type_ids, attention_mask, targets in tqdm(dataloader, total=len(dataloader)):\n","            input_ids = input_ids.to(device)\n","            token_type_ids = token_type_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            targets = targets.to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n","            train_targets.extend(targets.detach().cpu().numpy())\n","            loss = compute_loss(outputs, targets)\n","            model.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            train_losses.append(loss.detach().cpu().item())\n","        model.eval()\n","        valid_losses = []\n","        valid_preds = []\n","        valid_targets = []\n","        with torch.no_grad():\n","            for input_ids, token_type_ids, attention_mask, targets in tqdm(valid_dataloader, total=len(valid_dataloader)):\n","                input_ids = input_ids.to(device)\n","                token_type_ids = token_type_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","                targets = targets.to(device)\n","                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","                valid_preds.extend(outputs.sigmoid().cpu().numpy())\n","                valid_targets.extend(targets.cpu().numpy())\n","                loss = compute_loss(outputs, targets)\n","                valid_losses.append(loss.detach().cpu().item())\n","            valid_predictions.append(np.stack(valid_preds))\n","            test_preds = []\n","            for input_ids, token_type_ids, attention_mask in tqdm(test_dataloader, total=len(test_dataloader)):\n","                input_ids = input_ids.to(device)\n","                token_type_ids = token_type_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","                test_preds.extend(outputs.sigmoid().cpu().numpy())\n","            test_predictions.append(np.stack(test_preds))\n","            print()\n","        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n","        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n","            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n","            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) / len(valid_predictions)),\n","            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n","        ))\n","        print(\"\\t elapsed: {}s\".format(time.time() - start))\n","\n","    return valid_predictions, test_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Fold(object):\n","    def __init__(self, n_splits=5, shuffle=True, random_state=71):\n","        self.n_splits = n_splits\n","        self.shuffle = shuffle\n","        self.random_state = random_state\n","\n","    def get_groupkfold(self, train, group_name):\n","        group = train[group_name]\n","        unique_group = group.unique()\n","\n","        kf = KFold(\n","            n_splits=self.n_splits,\n","            shuffle=self.shuffle,\n","            random_state=self.random_state\n","        )\n","        folds_ids = []\n","        for trn_group_idx, val_group_idx in kf.split(unique_group):\n","            trn_group = unique_group[trn_group_idx]\n","            val_group = unique_group[val_group_idx]\n","            is_trn = group.isin(trn_group)\n","            is_val = group.isin(val_group)\n","            trn_idx = train[is_trn].index\n","            val_idx = train[is_val].index\n","            folds_ids.append((trn_idx, val_idx))\n","\n","        return folds_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gkf = Fold(n_splits=3, shuffle=True, random_state=71)\n","fold_ids = gkf.get_groupkfold(df_train, group_name=\"url\")\n","\n","for train_idx, valid_idx in fold_ids:\n","    print((df_train.loc[train_idx, \"question_type_spelling\"] > 0).sum())\n","    print((df_train.loc[valid_idx, \"question_type_spelling\"] > 0).sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["histories = []\n","test_dataset = torch.utils.data.TensorDataset(*test_inputs)\n","q_test_dataset = torch.utils.data.TensorDataset(*test_question_only_inputs)\n","\n","for fold, (train_idx, valid_idx) in enumerate(fold_ids):\n","    import gc\n","    gc.collect()\n","\n","    train_inputs = [inputs[i][train_idx] for i in range(3)]\n","    q_train_inputs = [question_only_inputs[i][train_idx] for i in range(3)]\n","    train_outputs = outputs[train_idx]\n","    train_dataset = torch.utils.data.TensorDataset(*train_inputs, train_outputs)\n","    q_train_dataset = torch.utils.data.TensorDataset(*q_train_inputs, train_outputs)\n","\n","    valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n","    q_valid_inputs = [question_only_inputs[i][valid_idx] for i in range(3)]\n","    valid_outputs = outputs[valid_idx]\n","    valid_dataset = torch.utils.data.TensorDataset(*valid_inputs, valid_outputs)\n","    q_valid_dataset = torch.utils.data.TensorDataset(*q_valid_inputs, valid_outputs)\n","\n","    history = train_and_predict(\n","        train_data=train_dataset, \n","        valid_data=valid_dataset,\n","        test_data=test_dataset, \n","        q_train_data=q_train_dataset, \n","        q_valid_data=q_valid_dataset,\n","        q_test_data=q_test_dataset, \n","        q_epochs=3, epochs=3, batch_size=8, fold=fold\n","        )\n","\n","    histories.append(history)"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Submit"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get val preds per each epochs\n","val_preds_list = []\n","n_epochs = len(histories[0][0])\n","\n","for epoch in range(n_epochs):\n","    val_preds_one_epoch = np.zeros([len(df_train), 30])    \n","\n","    for fold, (train_idx, valid_idx) in enumerate(fold_ids):\n","        val_pred = histories[fold][0][epoch]\n","        val_preds_one_epoch[valid_idx, :] += val_pred\n","\n","    val_preds_list.append(val_preds_one_epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["oof_predictions = np.zeros((n_epochs, len(df_train), len(output_categories)), dtype=np.float32)\n","\n","for j, name in enumerate(output_categories):\n","    for epoch in range(n_epochs):\n","        col = \"{}_{}\".format(epoch, name)\n","        oof_predictions[epoch, :, j] = val_preds_list[epoch][:, j]\n","\n","oof_predictions.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get test preds per each epochs\n","test_preds_list = []\n","\n","for epoch in range(n_epochs):\n","    test_preds_one_epoch = 0\n","\n","    for fold in range(len(fold_ids)):\n","        test_preds = histories[fold][1][epoch]\n","        test_preds_one_epoch += test_preds\n","\n","    test_preds_one_epoch = test_preds_one_epoch / len(fold_ids)\n","    test_preds_list.append(test_preds_one_epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_predictions = np.zeros((n_epochs, len(df_test), len(output_categories)), dtype=np.float32)\n","\n","for j, name in enumerate(output_categories):\n","    for epoch in range(n_epochs):\n","        col = \"{}_{}\".format(epoch, name)\n","        test_predictions[epoch, :, j] = test_preds_list[epoch][:, j]\n","\n","test_predictions.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from abc import abstractmethod\n","from sklearn.metrics import roc_auc_score\n","\n","\n","class Base_Model(object):\n","    @abstractmethod\n","    def fit(self, x_train, y_train, x_valid, y_valid, config):\n","        raise NotImplementedError\n","    \n","    @abstractmethod\n","    def get_best_iteration(self, model):\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def predict(self, model, features):\n","        raise NotImplementedError\n","        \n","    @abstractmethod\n","    def get_feature_importance(self, model):\n","        raise NotImplementedError      \n","        \n","\n","    def cv(self, y_train, train_features, test_features, feature_name, folds_ids, config):\n","        # initialize\n","        test_preds = np.zeros(len(test_features))\n","        oof_preds = np.zeros(len(train_features))\n","        importances = pd.DataFrame(index=feature_name)\n","        best_iteration = 0\n","        cv_score_list = []\n","        models = []\n","\n","        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n","            # get train data and valid data\n","            x_trn = train_features.iloc[trn_idx]\n","            y_trn = y_train[trn_idx]\n","            x_val = train_features.iloc[val_idx]\n","            y_val = y_train[val_idx]\n","            \n","            # train model\n","            model, best_score = self.fit(x_trn, y_trn, x_val, y_val, config)\n","            cv_score_list.append(best_score)\n","            models.append(model)\n","            best_iteration += self.get_best_iteration(model) / len(folds_ids)\n","    \n","            # predict out-of-fold and test\n","            oof_preds[val_idx] = self.predict(model, x_val)\n","            test_preds += self.predict(model, test_features) / len(folds_ids)\n","\n","            # get feature importances\n","            importances_tmp = pd.DataFrame(\n","                self.get_feature_importance(model),\n","                columns=[f'gain_{i_fold+1}'],\n","                index=feature_name\n","            )\n","            importances = importances.join(importances_tmp, how='inner')\n","\n","        # summary of feature importance\n","        feature_importance = importances.mean(axis=1)\n","\n","        # full train\n","        # model, best_score = self.full_train(train_features, y_train, config, best_iteration * 1.5)\n","        # oof_preds = self.predict(model, train_features)\n","        # test_preds = self.predict(model, test_features)\n","    \n","        evals_results = {\"evals_result\": {\n","            \"cv_score\": {f\"cv{i+1}\": cv_score for i, cv_score in enumerate(cv_score_list)},\n","            \"n_data\": len(train_features),\n","            \"best_iteration\": best_iteration,\n","            \"n_features\": len(train_features.columns),\n","            \"feature_importance\": feature_importance.sort_values(ascending=False).to_dict()\n","        }}\n","\n","        return models, oof_preds, test_preds, feature_importance, evals_results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def lgb_compute_spearmanr(preds, trues):\n","    rhos = spearmanr(trues.get_label(), preds).correlation\n","    return \"spearmanr\", rhos, True\n","\n","\n","def compute_spearmanr_each_col(trues, preds, n_bins=None):\n","    if n_bins:\n","        preds = binning_output(preds, n_bins)\n","    rhos = spearmanr(trues, preds).correlation\n","    return rhos"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import lightgbm as lgb\n","from pathlib import Path\n","\n","\n","class LightGBM(Base_Model):\n","    def fit(self, x_train, y_train, x_valid, y_valid, config):\n","        d_train = lgb.Dataset(x_train, label=y_train)\n","        d_valid = lgb.Dataset(x_valid, label=y_valid)\n","        lgb_model_params = config[\"model\"][\"model_params\"]\n","        lgb_train_params = config[\"model\"][\"train_params\"]\n","        model = lgb.train(\n","            params=lgb_model_params,\n","            train_set=d_train,\n","            valid_sets=[d_valid],\n","            valid_names=['valid'],\n","            feval=lgb_compute_spearmanr,\n","            **lgb_train_params\n","        )\n","        best_score = dict(model.best_score)\n","        return model, best_score\n","\n","    def full_train(self, x_train, y_train, config, iteration):\n","        d_train = lgb.Dataset(x_train, label=y_train)\n","        lgb_model_params = config[\"model\"][\"model_params\"]\n","        model = lgb.train(\n","            params=lgb_model_params,\n","            train_set=d_train,\n","            feval=lgb_compute_spearmanr,\n","            num_boost_round=int(iteration)\n","        )\n","        best_score = dict(model.best_score)\n","        return model, best_score\n","\n","    def get_best_iteration(self, model):\n","        return model.best_iteration\n","    \n","    def predict(self, model, features):\n","        return model.predict(features)\n","        \n","    def get_feature_importance(self, model):\n","        return model.feature_importance(importance_type='gain')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["config = {\n","    \"model\": {\n","        \"name\": \"lightgbm\",\n","        \"model_params\": {\n","            \"boosting_type\": \"gbdt\",\n","            \"objective\": \"rmse\",\n","            \"tree_learner\": \"serial\",\n","            \"learning_rate\": 0.1,\n","            \"max_depth\": 1,\n","            \"seed\": 71,\n","            \"bagging_seed\": 71,\n","            \"feature_fraction_seed\": 71,\n","            \"drop_seed\": 71,\n","            \"verbose\": -1\n","        },\n","        \"train_params\": {\n","            \"num_boost_round\": 5000,\n","            \"early_stopping_rounds\": 200,\n","            \"verbose_eval\": 500\n","        }\n","    }\n","}\n","\n","\n","outputs = compute_output_arrays(df_train)\n","oof_preds_list = []\n","test_preds_list = []\n","\n","for i_col in range(len(output_categories)):\n","    y_train = outputs[:, i_col]\n","    #x_train = pd.DataFrame(oof_predictions[:, :, 2].T)\n","    x_train = pd.DataFrame(np.concatenate([oof_predictions[:, :, i].T for i in range(30)], axis=1))\n","    x_test = pd.DataFrame(np.concatenate([test_predictions[:, :, i].T for i in range(30)], axis=1))\n","    feature_name = x_train.columns\n","\n","    model = LightGBM()\n","    models, oof_preds, test_preds, feature_importance, evals_results = model.cv(\n","            y_train, x_train, x_test, feature_name, fold_ids, config\n","    )\n","    oof_preds_list.append(oof_preds.reshape(-1, 1))\n","    test_preds_list.append(test_preds.reshape(-1, 1))\n","\n","    print(i_col, output_categories[i_col])\n","    print(compute_spearmanr_each_col(oof_preds, y_train))\n","    print(len(oof_preds), len(np.unique(oof_preds)))\n","    print(len(test_preds), len(np.unique(test_preds)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_spearmanr(trues, preds, n_bins=None):\n","    rhos = []\n","    if n_bins:\n","        preds = binning_output(preds, n_bins)\n","    for col_trues, col_pred in zip(trues.T, preds.T):\n","        if len(np.unique(col_pred)) == 1:\n","            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n","        rhos.append(spearmanr(col_trues, col_pred).correlation)\n","    return np.mean(rhos)\n","\n","\n","oof_preds_fi = np.concatenate(oof_preds_list, axis=1)\n","print(compute_spearmanr(outputs, oof_preds_fi))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_preds_fi = np.concatenate(test_preds_list, axis=1)\n","sub.iloc[:, 1:] = test_preds_fi\n","sub.to_csv('submission.csv', index=False)"]}],"metadata":{"interpreter":{"hash":"ac9e7467ab50678fb25b34fbfebaa7dd0935f663e602be01974fdf6c9ce75ada"},"kernelspec":{"display_name":"Python 3.10.4 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
